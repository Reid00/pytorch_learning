{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "一、线性回归模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 准备数据\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# sample number\n",
    "n = 400\n",
    "\n",
    "# 生成测试数据集\n",
    "X = 10 * torch.rand([n, 2]) - 5.0  # torch.randn 是均匀分布类型\n",
    "w0 = torch.tensor([[2.0], [-3.0]])\n",
    "b0 = torch.tensor([[10.0]])\n",
    "\n",
    "y = X @ w0 + b0 + torch.normal(0.0, 2.0, size=[n, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.scatter(X[:, 0].numpy(), y[:, 0].numpy(), c='b', label='samples')\n",
    "ax1.legend()\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y', rotation=0)\n",
    "\n",
    "ax2= plt.subplot(122)\n",
    "ax2.scatter(X[:, 1].numpy(), y[:, 0].numpy(), c='g', label='samples')\n",
    "ax2.legend()\n",
    "plt.xlabel('x2')\n",
    "plt.ylabel('y', rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.1990, -4.5614],\n        [-0.8613, -1.3396],\n        [-1.1601,  2.1362],\n        [-2.5265, -0.8018],\n        [ 2.1353, -0.2573],\n        [-0.4110, -2.0204],\n        [-1.9846,  4.0192],\n        [-1.8882,  0.1019]])\ntensor([[27.1468],\n        [12.6352],\n        [-1.3083],\n        [ 6.9933],\n        [14.7112],\n        [12.8322],\n        [-5.2764],\n        [ 5.9865]])\n"
     ]
    }
   ],
   "source": [
    "# 构建数据管道迭代器\n",
    "def data_iter(features, labels, batch_size=8):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    np.random.shuffle(indices)  #样本的读取顺序是随机的\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        indexs = torch.LongTensor(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield  features.index_select(0, indexs), labels.index_select(0, indexs)\n",
    "        \n",
    "# 测试数据管道效果   \n",
    "batch_size = 8\n",
    "(features,labels) = next(data_iter(X, y,batch_size))\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "source": [
    "2. 定义模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = torch.randn_like(w0, requires_grad=True)\n",
    "        self.b = torch.zeros_like(b0, requires_grad=True)\n",
    "\n",
    "    # 正向传播\n",
    "    def forward(self, x):\n",
    "        return x@self.w + self.b\n",
    "\n",
    "    # 损失函数\n",
    "    def loss_func(self, y_pred, y_true):\n",
    "        return torch.mean((y_pred-y_true)**2 / 2)\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "source": [
    "3. 训练模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, features, labels):\n",
    "\n",
    "    predictions = model.forward(features)\n",
    "    loss = model.loss_func(predictions, labels)\n",
    "\n",
    "    # 反向传播求梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用torch.no_grad() 避免梯度记录， 也可以通过操作model.w.data 实现避免梯度记录\n",
    "    with torch.no_grad():\n",
    "        # 梯度下降法更新参数\n",
    "        model.w -= 0.01 * model.w.grad\n",
    "        model.b -= 0.01 * model.b.grad\n",
    "\n",
    "        # 梯度清零\n",
    "        model.w.grad.zero_()\n",
    "        model.b.grad.zero_()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(41.0938, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 测试train_step 效果\n",
    "batch_size = 10\n",
    "features, labels = next(data_iter(X, y, batch_size))\n",
    "train_step(model, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch = 20 loss= 1.8846652507781982\n",
      "model.w = tensor([[ 1.9296],\n",
      "        [-2.9726]])\n",
      "model.b = tensor([[10.0611]])\n",
      "epoch = 40 loss= 2.7229256629943848\n",
      "model.w = tensor([[ 1.8898],\n",
      "        [-3.0089]])\n",
      "model.b = tensor([[10.0828]])\n",
      "epoch = 60 loss= 1.5944843292236328\n",
      "model.w = tensor([[ 1.9580],\n",
      "        [-3.0137]])\n",
      "model.b = tensor([[10.0838]])\n",
      "epoch = 80 loss= 2.19413161277771\n",
      "model.w = tensor([[ 1.9293],\n",
      "        [-2.9985]])\n",
      "model.b = tensor([[10.0710]])\n",
      "epoch = 100 loss= 2.581402063369751\n",
      "model.w = tensor([[ 1.9435],\n",
      "        [-3.0479]])\n",
      "model.b = tensor([[10.0680]])\n",
      "epoch = 120 loss= 1.622877836227417\n",
      "model.w = tensor([[ 1.9781],\n",
      "        [-3.0047]])\n",
      "model.b = tensor([[10.0785]])\n",
      "epoch = 140 loss= 5.3971638679504395\n",
      "model.w = tensor([[ 1.9310],\n",
      "        [-3.0045]])\n",
      "model.b = tensor([[10.0787]])\n",
      "epoch = 160 loss= 2.0983893871307373\n",
      "model.w = tensor([[ 1.9302],\n",
      "        [-3.0364]])\n",
      "model.b = tensor([[10.0666]])\n",
      "epoch = 180 loss= 1.9165757894515991\n",
      "model.w = tensor([[ 1.9326],\n",
      "        [-3.0498]])\n",
      "model.b = tensor([[10.0781]])\n",
      "epoch = 200 loss= 5.23592472076416\n",
      "model.w = tensor([[ 1.9807],\n",
      "        [-3.0160]])\n",
      "model.b = tensor([[10.0622]])\n",
      "epoch = 220 loss= 1.772748351097107\n",
      "model.w = tensor([[ 1.9542],\n",
      "        [-2.9840]])\n",
      "model.b = tensor([[10.0687]])\n",
      "epoch = 240 loss= 1.4329907894134521\n",
      "model.w = tensor([[ 1.9809],\n",
      "        [-2.9749]])\n",
      "model.b = tensor([[10.0790]])\n",
      "epoch = 260 loss= 1.7292516231536865\n",
      "model.w = tensor([[ 1.9178],\n",
      "        [-3.0106]])\n",
      "model.b = tensor([[10.0640]])\n",
      "epoch = 280 loss= 4.904874801635742\n",
      "model.w = tensor([[ 1.9037],\n",
      "        [-2.9882]])\n",
      "model.b = tensor([[10.0727]])\n",
      "epoch = 300 loss= 1.9646943807601929\n",
      "model.w = tensor([[ 1.9058],\n",
      "        [-3.0147]])\n",
      "model.b = tensor([[10.0682]])\n",
      "epoch = 320 loss= 1.3734276294708252\n",
      "model.w = tensor([[ 1.9478],\n",
      "        [-3.0337]])\n",
      "model.b = tensor([[10.0756]])\n",
      "epoch = 340 loss= 2.586533546447754\n",
      "model.w = tensor([[ 1.8802],\n",
      "        [-3.0149]])\n",
      "model.b = tensor([[10.0808]])\n",
      "epoch = 360 loss= 1.6615283489227295\n",
      "model.w = tensor([[ 1.9613],\n",
      "        [-2.9712]])\n",
      "model.b = tensor([[10.0813]])\n",
      "epoch = 380 loss= 2.9149200916290283\n",
      "model.w = tensor([[ 1.9142],\n",
      "        [-2.9844]])\n",
      "model.b = tensor([[10.0780]])\n",
      "epoch = 400 loss= 1.9541881084442139\n",
      "model.w = tensor([[ 1.9709],\n",
      "        [-3.0493]])\n",
      "model.b = tensor([[10.0689]])\n",
      "epoch = 420 loss= 1.5808546543121338\n",
      "model.w = tensor([[ 1.9561],\n",
      "        [-3.0721]])\n",
      "model.b = tensor([[10.0703]])\n",
      "epoch = 440 loss= 0.5072200894355774\n",
      "model.w = tensor([[ 1.9027],\n",
      "        [-3.0168]])\n",
      "model.b = tensor([[10.0677]])\n",
      "epoch = 460 loss= 3.938432216644287\n",
      "model.w = tensor([[ 1.9551],\n",
      "        [-3.0132]])\n",
      "model.b = tensor([[10.0697]])\n",
      "epoch = 480 loss= 2.1435205936431885\n",
      "model.w = tensor([[ 1.9116],\n",
      "        [-2.9635]])\n",
      "model.b = tensor([[10.0751]])\n",
      "epoch = 500 loss= 2.9589085578918457\n",
      "model.w = tensor([[ 1.9017],\n",
      "        [-2.9922]])\n",
      "model.b = tensor([[10.0636]])\n",
      "epoch = 520 loss= 1.6185458898544312\n",
      "model.w = tensor([[ 1.9748],\n",
      "        [-2.9627]])\n",
      "model.b = tensor([[10.0619]])\n",
      "epoch = 540 loss= 2.771918773651123\n",
      "model.w = tensor([[ 1.9519],\n",
      "        [-3.0698]])\n",
      "model.b = tensor([[10.0662]])\n",
      "epoch = 560 loss= 2.102234363555908\n",
      "model.w = tensor([[ 1.9480],\n",
      "        [-2.9982]])\n",
      "model.b = tensor([[10.0659]])\n",
      "epoch = 580 loss= 1.0824239253997803\n",
      "model.w = tensor([[ 1.9668],\n",
      "        [-3.0001]])\n",
      "model.b = tensor([[10.0741]])\n",
      "epoch = 600 loss= 2.408041477203369\n",
      "model.w = tensor([[ 1.9476],\n",
      "        [-2.9841]])\n",
      "model.b = tensor([[10.0703]])\n",
      "epoch = 620 loss= 1.7426674365997314\n",
      "model.w = tensor([[ 1.9612],\n",
      "        [-3.0324]])\n",
      "model.b = tensor([[10.0661]])\n",
      "epoch = 640 loss= 3.291041851043701\n",
      "model.w = tensor([[ 1.9097],\n",
      "        [-2.9735]])\n",
      "model.b = tensor([[10.0697]])\n",
      "epoch = 660 loss= 1.155871868133545\n",
      "model.w = tensor([[ 1.9022],\n",
      "        [-2.9935]])\n",
      "model.b = tensor([[10.0762]])\n",
      "epoch = 680 loss= 4.694533348083496\n",
      "model.w = tensor([[ 1.9486],\n",
      "        [-3.0674]])\n",
      "model.b = tensor([[10.0628]])\n",
      "epoch = 700 loss= 1.3980706930160522\n",
      "model.w = tensor([[ 1.9478],\n",
      "        [-3.0041]])\n",
      "model.b = tensor([[10.0750]])\n",
      "epoch = 720 loss= 1.279647707939148\n",
      "model.w = tensor([[ 1.8998],\n",
      "        [-3.0009]])\n",
      "model.b = tensor([[10.0803]])\n",
      "epoch = 740 loss= 2.87496018409729\n",
      "model.w = tensor([[ 1.9588],\n",
      "        [-3.0119]])\n",
      "model.b = tensor([[10.0679]])\n",
      "epoch = 760 loss= 0.9619500041007996\n",
      "model.w = tensor([[ 1.8914],\n",
      "        [-3.0338]])\n",
      "model.b = tensor([[10.0749]])\n",
      "epoch = 780 loss= 3.675400495529175\n",
      "model.w = tensor([[ 1.9679],\n",
      "        [-3.0296]])\n",
      "model.b = tensor([[10.0680]])\n",
      "epoch = 800 loss= 3.5891196727752686\n",
      "model.w = tensor([[ 1.9760],\n",
      "        [-3.0513]])\n",
      "model.b = tensor([[10.0820]])\n",
      "epoch = 820 loss= 2.542764663696289\n",
      "model.w = tensor([[ 1.9751],\n",
      "        [-3.0263]])\n",
      "model.b = tensor([[10.0661]])\n",
      "epoch = 840 loss= 1.3655163049697876\n",
      "model.w = tensor([[ 1.9212],\n",
      "        [-3.0231]])\n",
      "model.b = tensor([[10.0793]])\n",
      "epoch = 860 loss= 0.976056694984436\n",
      "model.w = tensor([[ 1.9301],\n",
      "        [-2.9783]])\n",
      "model.b = tensor([[10.0708]])\n",
      "epoch = 880 loss= 2.1202268600463867\n",
      "model.w = tensor([[ 1.9254],\n",
      "        [-2.9890]])\n",
      "model.b = tensor([[10.0741]])\n",
      "epoch = 900 loss= 2.259918451309204\n",
      "model.w = tensor([[ 2.0099],\n",
      "        [-3.0141]])\n",
      "model.b = tensor([[10.0782]])\n",
      "epoch = 920 loss= 1.2278465032577515\n",
      "model.w = tensor([[ 1.9432],\n",
      "        [-2.9966]])\n",
      "model.b = tensor([[10.0690]])\n",
      "epoch = 940 loss= 0.9949313402175903\n",
      "model.w = tensor([[ 1.9682],\n",
      "        [-3.0095]])\n",
      "model.b = tensor([[10.0691]])\n",
      "epoch = 960 loss= 0.6948347687721252\n",
      "model.w = tensor([[ 1.9017],\n",
      "        [-3.0235]])\n",
      "model.b = tensor([[10.0696]])\n",
      "epoch = 980 loss= 2.560584783554077\n",
      "model.w = tensor([[ 1.9244],\n",
      "        [-3.0374]])\n",
      "model.b = tensor([[10.0718]])\n",
      "epoch = 1000 loss= 2.9537618160247803\n",
      "model.w = tensor([[ 1.9341],\n",
      "        [-2.9679]])\n",
      "model.b = tensor([[10.0746]])\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, epochs):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for features, labels in data_iter(X, y, 10):\n",
    "            loss = train_step(model, features, labels)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print('epoch =', epoch, 'loss=', loss.item())\n",
    "            print('model.w =', model.w.data)\n",
    "            print('model.b =', model.b.data)\n",
    "\n",
    "train_model(model, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果可视化\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.figure(figsize = (12,5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.scatter(X[:,0].numpy(),Y[:,0].numpy(), c = \"b\",label = \"samples\")\n",
    "ax1.plot(X[:,0].numpy(),(model.w[0].data*X[:,0]+model.b[0].data).numpy(),\"-r\",linewidth = 5.0,label = \"model\")\n",
    "ax1.legend()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"y\",rotation = 0)\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.scatter(X[:,1].numpy(),Y[:,0].numpy(), c = \"g\",label = \"samples\")\n",
    "ax2.plot(X[:,1].numpy(),(model.w[1].data*X[:,1]+model.b[0].data).numpy(),\"-r\",linewidth = 5.0,label = \"model\")\n",
    "ax2.legend()\n",
    "plt.xlabel(\"x2\")\n",
    "plt.ylabel(\"y\",rotation = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "二、 DNN 二分类模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}