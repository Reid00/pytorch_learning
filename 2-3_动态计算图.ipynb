{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "动态计算图\n",
    "本节我们将介绍 Pytorch的动态计算图。\n",
    "\n",
    "包括：\n",
    "\n",
    "动态计算图简介\n",
    "\n",
    "计算图中的Function\n",
    "\n",
    "计算图和反向传播\n",
    "\n",
    "叶子节点和非叶子节点\n",
    "\n",
    "计算图在TensorBoard中的可视化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Pytorch的计算图由节点和边组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系。\n",
    "\n",
    "Pytorch中的计算图是动态图。这里的动态主要有两重含义。\n",
    "\n",
    "第一层含义是：计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。\n",
    "\n",
    "第二层含义是：计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[3., 1.]], requires_grad=True), torch.Size([1, 2]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# 1，计算图的正向传播是立即执行的。\n",
    "\n",
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0, 1.0]], requires_grad=True)\n",
    "w, w.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.4171,  1.0689],\n        [-0.4689, -0.4241],\n        [-0.5173, -0.6627],\n        [-0.5289, -0.0475],\n        [ 0.2644,  0.7916],\n        [-0.1885,  0.0868],\n        [ 2.0494,  1.1659],\n        [-1.0164,  0.5558],\n        [-0.3946,  1.0539],\n        [-1.1730, -1.4380]])\ntensor([[3.],\n        [1.]], grad_fn=<TBackward>)\ntensor(22.2578)\ntensor([[ 8.3202],\n        [ 1.1692],\n        [ 0.7853],\n        [ 1.3657],\n        [ 4.5849],\n        [ 2.5213],\n        [10.3142],\n        [ 0.5065],\n        [ 2.8701],\n        [-1.9571]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "\n",
    "x = torch.randn(10,2)\n",
    "y = torch.randn(10,1)\n",
    "\n",
    "y_hat = x@w.t() + b # Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关\n",
    "loss = torch.mean(torch.pow(y_hat -y, 2))\n",
    "print(x)\n",
    "print(w.t())\n",
    "print(loss.data)\n",
    "print(y_hat.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2，计算图在反向传播后立即销毁。\n",
    "\n",
    "w = torch.tensor([[3.0, 1.0]], requires_grad=True)\n",
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "\n",
    "x = torch.randn(10,2)\n",
    "y = torch.randn(10,1)\n",
    "\n",
    "y_hat = x@w.t() + b\n",
    "loss = torch.mean(torch.pow(y_hat-y, 2))\n",
    "\n",
    "#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True\n",
    "loss.backward()  #loss.backward(retain_graph = True) \n"
   ]
  },
  {
   "source": [
    "二、计算图中的Function\n",
    "计算图中的 张量我们已经比较熟悉了, 计算图中的另外一种节点是Function, 实际上就是 Pytorch中各种对张量操作的函数。\n",
    "\n",
    "这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑。\n",
    "\n",
    "我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLu(torch.autograd.Function):\n",
    "    # 正向传播逻辑，可以用ctx 存储一些值，供反向传播使用\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    #反向传播逻辑\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[4.5000, 4.5000]])\ntensor([[4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0, 1.0]], requires_grad=True)\n",
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "x = torch.tensor([[-1.0, -1.0], [1.0, 1.0]])\n",
    "y = torch.tensor([[2.0, 3.0]])\n",
    "\n",
    "relu = MyReLu.apply  # relu现在也可以具有正向传播和反向传播功能\n",
    "y_hat = relu(x@w.t() + b)\n",
    "loss = torch.mean(torch.pow(y_hat-y, 2))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "source": [
    "三、 计算图与反向传播\n",
    "\n",
    "了解了Function的功能，我们可以简单地理解一下反向传播的原理和过程。理解该部分原理需要一些高等数学中求导链式法则的基础知识。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y1 = x + 1\n",
    "y2 = 2 * x\n",
    "\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "loss.backward()\n",
    "loss.item()\n",
    "\n",
    "# loss.backward()语句调用后，依次发生以下计算过程。\n",
    "\n",
    "# 1，loss自己的grad梯度赋值为1，即对自身的梯度为1。  loss_y1 = 2y1  / loss_y2 = 2y2\n",
    "\n",
    "# 2，loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad 和y2.grad。y1_x = 1 // y2_x = 2\n",
    "\n",
    "# 3，y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。\n",
    "\n",
    "# （注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）\n",
    "\n",
    "# 正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。"
   ]
  },
  {
   "source": [
    "四、叶子节点和非叶子节点\n",
    "执行下面代码，我们会发现 loss.grad并不是我们期望的1,而是 None。\n",
    "\n",
    "类似地 y1.grad 以及 y2.grad也是 None.\n",
    "\n",
    "这是为什么呢？这是由于它们不是叶子节点张量。\n",
    "\n",
    "在反向传播过程中，只有 is_leaf=True 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。\n",
    "\n",
    "那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。\n",
    "\n",
    "1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。\n",
    "\n",
    "2，叶子节点张量的 requires_grad属性必须为True.\n",
    "\n",
    "Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。\n",
    "\n",
    "所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。\n",
    "\n",
    "如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss.grad \t None\ny1.grad \t None\ny2.gard \t None\ntensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y1 = x + 1 \n",
    "y2 = 2 * x\n",
    "loss = (y1 - y2)**2\n",
    "\n",
    "loss.backward()\n",
    "print('loss.grad \\t', loss.grad)\n",
    "print('y1.grad \\t', y1.grad)\n",
    "print('y2.gard \\t', y2.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "source": [
    "利用retain_grad可以保留非叶子节点的梯度值，利用register_hook可以查看非叶子节点的梯度值。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y2 grad:  tensor(4.)\ny1 grad:  tensor(-4.)\nloss.grad: tensor(1.)\nx.grad: tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "#正向传播\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "#非叶子节点梯度显示控制\n",
    "y1.register_hook(lambda grad: print('y1 grad: ', grad))\n",
    "y2.register_hook(lambda grad: print('y2 grad: ', grad))\n",
    "loss.retain_grad()\n",
    "\n",
    "#反向传播\n",
    "loss.backward()\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(\"x.grad:\", x.grad)"
   ]
  },
  {
   "source": [
    "五、计算图在TensorBoard中的可视化\n",
    "可以利用 torch.utils.tensorboard 将计算图导出到 TensorBoard进行可视化。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(2, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x@self.w + self.b\n",
    "        return y\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "writer.add_graph(net, input_to_model=torch.rand(10, 2))\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No known TensorBoard instances running.\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ERROR: Could not find `tensorboard`. Please ensure that your PATH\ncontains an executable `tensorboard` program, or explicitly specify\nthe path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\nenvironment variable."
     },
     "metadata": {}
    }
   ],
   "source": [
    "notebook.start(\"--logdir ./data/tensorboard\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}